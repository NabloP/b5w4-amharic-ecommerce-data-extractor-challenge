{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19119a93",
   "metadata": {},
   "source": [
    "# üõí Task 1 ‚Äì Data Ingestion & Preprocessing  \n",
    "üìò Version: 2025-06-24\n",
    "\n",
    "Programmatic data scraping and preprocessing of Amharic e-commerce posts from Telegram. This notebook connects to public Telegram channels, extracts messages and metadata (e.g., views, timestamps), and performs text normalization to prepare a clean dataset for Named Entity Recognition (NER) labeling.\n",
    "\n",
    "---\n",
    "\n",
    "**Challenge:** B5W4 ‚Äì Amharic E-Commerce Data Extractor  \n",
    "**Company:** EthioMart (Telegram E-Commerce Aggregator)  \n",
    "**Author:** Nabil Mohamed  \n",
    "**Branch:** `task-1-ingestion-cleaning`  \n",
    "**Date:** June 2025  \n",
    "\n",
    "---\n",
    "\n",
    "### üìå This notebook covers:\n",
    "- API connection to 5+ Amharic Telegram vendor channels\n",
    "- Ingestion of messages, views, and timestamps\n",
    "- Basic Amharic-friendly text normalization and filtering\n",
    "- Structured saving of cleaned messages for Task 2 labeling\n",
    "- Output saved to: `data/cleaned/telegram_messages.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "466b39a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Changed working directory to project root\n",
      "‚úÖ Added to sys.path: c:\\Users\\admin\\Documents\\GIT Repositories\\b5w4-amharic-ecommerce-data-extractor-challenge\n",
      "üìÅ Output path ready\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# üõ† Ensure Notebook Runs from Project Root (for src/ imports to work)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# If running from /notebooks/, move up to project root\n",
    "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "    os.chdir(\"..\")\n",
    "    print(\"üìÇ Changed working directory to project root\")\n",
    "\n",
    "# Add project root to sys.path so `src/` modules can be imported\n",
    "project_root = os.getcwd()\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    print(f\"‚úÖ Added to sys.path: {project_root}\")\n",
    "\n",
    "# Optional: verify file presence to confirm we're in the right place\n",
    "expected_path = \"data/raw\"\n",
    "print(\n",
    "    \"üìÅ Output path ready\"\n",
    "    if os.path.exists(expected_path)\n",
    "    else f\"‚ö†Ô∏è Output path not found: {expected_path}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aae136",
   "metadata": {},
   "source": [
    "## üì¶ Imports & Environment Setup\n",
    "\n",
    "This cell loads core libraries required for data ingestion, text cleaning, and structured saving. The imports are grouped by function:\n",
    "\n",
    "- **Data handling**: `pandas` for tabular processing, `re` for text cleaning, `datetime` for timestamp formatting  \n",
    "- **Telegram scraping**: `telethon` for connecting to public Telegram channels and retrieving post history  \n",
    "- **Environment management**: `dotenv` to securely load API credentials from a `.env` file  \n",
    "\n",
    "These tools form the backbone of your ingestion pipeline and will be reused throughout the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3246ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# üì¶ Core Imports ‚Äì Data Ingestion, Cleaning, and Saving\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Standard Library\n",
    "import os  # File and path handling\n",
    "import re  # Regex for text normalization\n",
    "from datetime import datetime  # Timestamp formatting\n",
    "import warnings  # Suppress benign warnings\n",
    "\n",
    "# Core Analysis\n",
    "import pandas as pd  # Structured data handling\n",
    "\n",
    "# Optional: tidy up output\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4ae579",
   "metadata": {},
   "source": [
    "## üì° Telegram API Client Initialization\n",
    "\n",
    "This section sets up a secure connection to Telegram using the `Telethon` library. API credentials (`TELEGRAM_API_ID`, `TELEGRAM_API_HASH`) are loaded from a `.env` file.\n",
    "\n",
    "The script checks for missing credentials and handles connection errors gracefully. If successful, a `TelegramClient` is initialized under the session name `\"ethio_ingestor\"`, ready to fetch message histories from e-commerce channels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d47057d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Telegram client initialized.\n"
     ]
    }
   ],
   "source": [
    "from telethon.sync import TelegramClient\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API credentials\n",
    "load_dotenv()\n",
    "\n",
    "api_id = os.getenv(\"TELEGRAM_API_ID\")\n",
    "api_hash = os.getenv(\"TELEGRAM_API_HASH\")\n",
    "\n",
    "# Verify that credentials are present\n",
    "if not api_id or not api_hash:\n",
    "    raise ValueError(\"‚ùå API credentials not found. Please check your .env file.\")\n",
    "\n",
    "# Initialize the Telegram client\n",
    "try:\n",
    "    client = TelegramClient(\"ethio_ingestor\", api_id, api_hash)\n",
    "    print(\"‚úÖ Telegram client initialized.\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Failed to initialize Telegram client.\")\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7481a34e",
   "metadata": {},
   "source": [
    "## üß≤ Channel Selection & Message Scraping\n",
    "\n",
    "This section defines a list of target Telegram vendor channels and fetches recent messages from each using the `GetHistoryRequest` method.\n",
    "\n",
    "For each message, the script extracts:\n",
    "- Raw text (`message`)\n",
    "- View count (`views`)\n",
    "- Timestamp (`date`)\n",
    "- Channel name\n",
    "\n",
    "Results are stored in a structured format (list of dictionaries), which will later be converted into a pandas DataFrame for cleaning and analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb59690",
   "metadata": {},
   "source": [
    "## üîê Step 1: Request Telegram Login Code\n",
    "\n",
    "To authorize your client, you'll need to log in using your Telegram phone number. This step sends a verification code to your Telegram app (not via SMS).\n",
    "\n",
    "**Instructions:**\n",
    "- Enter your phone number in international format (e.g., `+2519XXXXXXXX`)\n",
    "- Telegram will send you a 5-digit code via your **Telegram app** (not SMS)\n",
    "- You'll use that code in the next step to complete the login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba5948a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Code sent. Please check your Telegram app for the verification code.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Server closed the connection: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Attempt 1 at connecting failed: TimeoutError: \n",
      "Attempt 2 at connecting failed: ConnectionAbortedError: [Errno 10053] Connect call failed ('149.154.167.91', 443)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# üîê Request Login Code from Telegram\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Replace with your phone number in international format\n",
    "phone_number = \"+251711029700\"\n",
    "\n",
    "# Send the code to your Telegram app\n",
    "await client.send_code_request(phone_number)\n",
    "\n",
    "print(\"‚úÖ Code sent. Please check your Telegram app for the verification code.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a982574c",
   "metadata": {},
   "source": [
    "## ‚úÖ Step 2: Sign In Using the Code\n",
    "\n",
    "Once you've received your 5-digit verification code from the Telegram app:\n",
    "\n",
    "1. Paste the code into the next code cell (replace `'12345'`)\n",
    "2. Run the cell to complete the sign-in\n",
    "3. This session will be cached, so you won‚Äôt need to do this again unless you delete your `.session` file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0728d875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ Authorization successful. You're now logged in.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# ‚úÖ Sign In with Verification Code\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Replace with the code sent to your Telegram app\n",
    "verification_code = \"31373\"\n",
    "\n",
    "# Complete sign-in\n",
    "await client.sign_in(phone_number, code=verification_code)\n",
    "\n",
    "print(\"üéâ Authorization successful. You're now logged in.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2da31f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ZemenExpress: Collected 100/500\n",
      "‚úÖ ZemenExpress: Collected 200/500\n",
      "‚úÖ ZemenExpress: Collected 300/500\n",
      "‚úÖ ZemenExpress: Collected 400/500\n",
      "‚úÖ ZemenExpress: Collected 500/500\n",
      "‚úÖ Shageronlinestore: Collected 100/500\n",
      "‚úÖ Shageronlinestore: Collected 200/500\n",
      "‚úÖ Shageronlinestore: Collected 300/500\n",
      "‚úÖ Shageronlinestore: Collected 400/500\n",
      "‚úÖ Shageronlinestore: Collected 500/500\n",
      "‚úÖ Leyueqa: Collected 100/500\n",
      "‚úÖ Leyueqa: Collected 200/500\n",
      "‚úÖ Leyueqa: Collected 300/500\n",
      "‚úÖ Leyueqa: Collected 400/500\n",
      "‚úÖ Leyueqa: Collected 500/500\n",
      "‚úÖ marakibrand: Collected 100/500\n",
      "‚úÖ marakibrand: Collected 200/500\n",
      "‚úÖ marakibrand: Collected 300/500\n",
      "‚úÖ marakibrand: Collected 400/500\n",
      "‚úÖ marakibrand: Collected 500/500\n",
      "‚úÖ MerttEka: Collected 100/500\n",
      "‚úÖ MerttEka: Collected 200/500\n",
      "‚úÖ MerttEka: Collected 300/500\n",
      "‚úÖ MerttEka: Collected 400/500\n",
      "‚úÖ MerttEka: Collected 500/500\n",
      "‚úÖ Fashiontera: Collected 100/500\n",
      "‚úÖ Fashiontera: Collected 200/500\n",
      "‚úÖ Fashiontera: Collected 300/500\n",
      "‚úÖ Fashiontera: Collected 400/500\n",
      "‚úÖ Fashiontera: Collected 500/500\n",
      "‚úÖ nevacomputer: Collected 100/500\n",
      "‚úÖ nevacomputer: Collected 200/500\n",
      "‚úÖ nevacomputer: Collected 300/500\n",
      "‚úÖ nevacomputer: Collected 400/500\n",
      "‚úÖ nevacomputer: Collected 500/500\n",
      "‚úÖ ethio_brand_collection: Collected 100/500\n",
      "‚úÖ ethio_brand_collection: Collected 200/500\n",
      "‚úÖ ethio_brand_collection: Collected 300/500\n",
      "‚úÖ ethio_brand_collection: Collected 400/500\n",
      "‚úÖ ethio_brand_collection: Collected 500/500\n",
      "‚úÖ Shewabrand: Collected 100/500\n",
      "‚úÖ Shewabrand: Collected 200/500\n",
      "‚úÖ Shewabrand: Collected 300/500\n",
      "‚úÖ Shewabrand: Collected 400/500\n",
      "‚úÖ Shewabrand: Collected 500/500\n",
      "‚úÖ sinayelj: Collected 100/500\n",
      "‚úÖ sinayelj: Collected 200/500\n",
      "‚úÖ sinayelj: Collected 300/500\n",
      "‚úÖ sinayelj: Collected 400/500\n",
      "‚úÖ sinayelj: Collected 500/500\n",
      "\n",
      "üéØ Total messages scraped: 2497\n",
      "üìÅ Messages saved to: C:\\Users\\admin\\Documents\\GIT Repositories\\b5w4-amharic-ecommerce-data-extractor-challenge\\data\\raw\\telegram_messages_raw.csv\n"
     ]
    }
   ],
   "source": [
    "from telethon.tl.functions.messages import GetHistoryRequest\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure client is connected\n",
    "await client.connect()\n",
    "\n",
    "if not await client.is_user_authorized():\n",
    "    print(\"üîê You're not authorized. This client may require login with code.\")\n",
    "\n",
    "# Define channels and fetch limits\n",
    "channel_usernames = [\n",
    "    \"ZemenExpress\",\n",
    "    \"Shageronlinestore\",\n",
    "    \"Leyueqa\",\n",
    "    \"marakibrand\",\n",
    "    \"MerttEka\",\n",
    "    \"Fashiontera\",\n",
    "    \"nevacomputer\",\n",
    "    \"ethio_brand_collection\",\n",
    "    \"Shewabrand\",\n",
    "    \"sinayelj\",\n",
    "]\n",
    "\n",
    "total_limit = 500  # Number of messages to fetch per channel\n",
    "batch_size = 100  # Telegram max per request\n",
    "all_messages = []  # Container for results\n",
    "\n",
    "# Iterate over each vendor channel\n",
    "for username in channel_usernames:\n",
    "    offset_id = 0\n",
    "    collected = 0\n",
    "\n",
    "    while collected < total_limit:\n",
    "        try:\n",
    "            entity = await client.get_entity(username)\n",
    "            history = await client(\n",
    "                GetHistoryRequest(\n",
    "                    peer=entity,\n",
    "                    limit=batch_size,\n",
    "                    offset_date=None,\n",
    "                    offset_id=offset_id,\n",
    "                    max_id=0,\n",
    "                    min_id=0,\n",
    "                    add_offset=0,\n",
    "                    hash=0,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            messages = history.messages\n",
    "            if not messages:\n",
    "                break  # No more to fetch\n",
    "\n",
    "            for msg in messages:\n",
    "                if msg.message:\n",
    "                    all_messages.append(\n",
    "                        {\n",
    "                            \"channel\": username,\n",
    "                            \"message\": msg.message,\n",
    "                            \"views\": msg.views,\n",
    "                            \"timestamp\": msg.date.isoformat(),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            offset_id = messages[-1].id\n",
    "            collected += len(messages)\n",
    "            print(f\"‚úÖ {username}: Collected {collected}/{total_limit}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to fetch from {username} ‚Äî {str(e)}\")\n",
    "            break\n",
    "\n",
    "print(f\"\\nüéØ Total messages scraped: {len(all_messages)}\")\n",
    "\n",
    "# Save to CSV\n",
    "output_path = Path(\"data/raw/telegram_messages_raw.csv\")\n",
    "df = pd.DataFrame(all_messages)\n",
    "df.dropna(subset=[\"message\"], inplace=True)\n",
    "df.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"üìÅ Messages saved to: {output_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f717550a",
   "metadata": {},
   "source": [
    "## üßº Clean & Normalize Telegram Messages\n",
    "\n",
    "This step prepares raw messages for Named Entity Recognition (NER) labeling by applying Amharic-specific text cleaning and formatting.\n",
    "\n",
    "The cleaning logic includes:\n",
    "- Removing emojis, special characters, and noisy symbols\n",
    "- Retaining Amharic (`\\u1200-\\u137F`) characters, basic punctuation, and digits\n",
    "- Lowercasing and normalizing whitespace\n",
    "\n",
    "Cleaned messages are saved to `data/cleaned/telegram_messages_cleaned.csv` and are ready for manual annotation in CoNLL format (Task 2).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f61144b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned messages saved to: C:\\Users\\admin\\Documents\\GIT Repositories\\b5w4-amharic-ecommerce-data-extractor-challenge\\data\\cleaned\\telegram_messages_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# üßº Clean and Normalize Telegram Messages for Labeling\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Load raw messages\n",
    "raw_path = Path(\"data/raw/telegram_messages_raw.csv\")\n",
    "df = pd.read_csv(raw_path)\n",
    "\n",
    "\n",
    "# Define Amharic-preserving cleaner\n",
    "def clean_text(text):\n",
    "    # Retain Amharic characters, basic Latin text, digits, and punctuation\n",
    "    cleaned = re.sub(r\"[^\\u1200-\\u137F·ç°·ç¢\\dA-Za-z.,:!?\\\\s]\", \"\", str(text))\n",
    "    cleaned = re.sub(r\"\\s+\", \" \", cleaned)  # collapse excessive spacing\n",
    "    return cleaned.strip().lower()\n",
    "\n",
    "\n",
    "# Apply cleaning function\n",
    "df[\"cleaned_message\"] = df[\"message\"].apply(clean_text)\n",
    "\n",
    "# Save cleaned output\n",
    "cleaned_path = Path(\"data/cleaned/telegram_messages_cleaned.csv\")\n",
    "df.to_csv(cleaned_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"‚úÖ Cleaned messages saved to: {cleaned_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4ad9f2",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è Select Messages for CoNLL Labeling\n",
    "\n",
    "To begin manual annotation, we‚Äôll extract a representative sample of cleaned messages.  \n",
    "These messages will be saved in a plain text `.txt` file where each line is one message ‚Äî making it easier to tokenize and label manually.\n",
    "\n",
    "We'll target 30‚Äì50 diverse examples that are rich in product names, prices, and locations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8827ade4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Sampled messages saved for labeling at: C:\\Users\\admin\\Documents\\GIT Repositories\\b5w4-amharic-ecommerce-data-extractor-challenge\\data\\labeled\\candidate_messages_for_labeling.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "cleaned_message",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "6d59fd09-e8c7-4c1d-89e2-4ed1a8e19343",
       "rows": [
        [
         "1178",
         "nikeairforcemadeinvietnamsize41,42price3900freedeliveryinboxhiwe5266·àµ·àç·ä≠251945355266·çã·àΩ·äï·â∞·à´fashiontera·ä†·ãµ·à´·àª:·ä†·ã≤·àµ·ä†·â†·â£,·å¶·à≠·àÄ·ã≠·àé·âΩ·ãµ·à™·àù·â≥·ãà·à≠2·â∞·äõ·çé·âÖ·â¢·àÆ·âÅ·å•·à≠205"
        ],
        [
         "872",
         "2in1eggslicer·ã®·â∞·âÄ·âÄ·àà·ä•·äï·âÅ·àã·àç·äì·ãµ·äï·âΩ·àò·à∞·äï·å†·âÇ·ã´·àÅ·àà·âµ·ä†·ã≠·äê·âµ·ä†·âÜ·à´·à®·å•·çÖ·ãµ·âµ·ã´·àà·àù·åç·â•·ãù·åç·åÖ·âµ800·â•·à≠a.adelivery100·â•·à≠·â•·âª·àò·ä™·äì·àõ·âÜ·àö·ã´·ä®·â∞·â∏·åà·à©·ã≠·ã∞·ãç·àâ·àç·äï·ä•·äì·âÄ·â•·àç·ãé·â≥·àà·äï0944222324ordermertteka10904944848ordermertteka2·àû·â£·ã≠·àç·â£·äï·ä™·äï·åç·ä•·äï·å†·âÄ·àõ·àà·äï·ä†·ãµ·à´·àª·âΩ·äï·àò·åà·äì·äõ·ãò·çç·àò·àΩ·åç·à´·äï·ãµ·àû·àç3·äõ·çé·âÖ·ä®·àä·çç·âµ·à≤·ãà·à≠·ã±·ãà·ã∞·âÄ·äù·â≥·å•·çà·ãç·âÄ·å•·â≥376tiktoktiktok.commirttekatelegramt.memertteka·àà·ãà·ã≥·åÜ·âΩ·ãéforward·ã´·ãµ·à≠·åâ"
        ],
        [
         "2003",
         "·àµ·àà·ãö·àÖ·àù·â†·àò·àµ·âÄ·àç·àã·ã≠·à≥·àà·ä®·å•·äï·âµ·åÄ·àù·àÆ·åå·â≥·â†·àò·âÉ·â•·à≠·ã´·àâ·àô·â≥·äï·äï·àÅ·àâ·ä†·àµ·äê·à£·àÉ·ã≠·àõ·äñ·â∞·ä†·â†·ãç·âÖ·ã±·àµ·çä·àç·ä≠·àµ·ãâ·ãµ·ã®·à∏·ãã·â•·à´·äï·ãµ·ã®·ä≠·à≠·àµ·âµ·äì·ä•·àù·äê·âµ·â∞·ä®·â≥·ãÆ·âΩ·ä•·äï·ä≥·äï·àà·åå·â≥·âΩ·äï·àà·àò·ãµ·àê·äí·â≥·âΩ·äï·àà·ä¢·ã®·à±·àµ·ä≠·à≠·àµ·â∂·àµ·ã®·àµ·âÖ·àà·âµ·ä•·äì·ã®·âµ·äï·à≥·ä§·â†·ãì·àç·â†·à∞·àã·àù·ä†·ã∞·à®·à≥·âΩ·àÅ·ä†·ã∞·à®·à∞·äï::"
        ],
        [
         "2200",
         "niketechherasize4041424344madeinvietnamshewabrand·ä†·ãµ·à´·àª·ãµ·à¨·ã≥·ãã·ä†·à∏·ãã·àö·äì·àÖ·äï·çÉ1·äõ·çé·âÖ·àã·ã≠·ä•·äï·åà·äõ·àà·äï·ã®·â¥·àå·åç·à´·àù·âª·äì·àã·âΩ·äï·äï·ã≠·âÄ·àã·âÄ·àâhttps:t.meshewabrandhttps:t.meshewabrandhttps:t.meshewabrandhttps:t.meshewabrand·ã®·â§·âµ·âÅ·å•·à≠109·ä•·äì110·â†inboxshewat2·ä†·ãã·à©·äï09873364580948595409·ã≠·ã∞·ãç·àâ·àç·äï"
        ],
        [
         "843",
         "givenchysize404142434445price:8000brfreedeliveryinbox:maraki2211·àµ·àç·ä≠:251913321831·ä†·ãµ·à´·àª·ä†·ã≤·àµ·ä†·â†·â£,·àú·ä≠·à≤·äÆ·ç°·ä®·ä¨·ä¨·à≠·àÖ·äï·åª50·àú·ãà·à®·ãµ·â•·àé·ä†·ã≠·àò·äï·àÖ·äï·çÉ·åç·à´·ãç·äï·ãµ·çç·àé·à≠·àã·ã≠·ç°·ã®·à±·âÖ·âÅ.012marakibrand·àõ·à´·ä™·â•·à´·äï·ãµ"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 5
       }
      },
      "text/plain": [
       "1178    nikeairforcemadeinvietnamsize41,42price3900fre...\n",
       "872     2in1eggslicer·ã®·â∞·âÄ·âÄ·àà·ä•·äï·âÅ·àã·àç·äì·ãµ·äï·âΩ·àò·à∞·äï·å†·âÇ·ã´·àÅ·àà·âµ·ä†·ã≠·äê·âµ·ä†·âÜ·à´·à®·å•·çÖ...\n",
       "2003    ·àµ·àà·ãö·àÖ·àù·â†·àò·àµ·âÄ·àç·àã·ã≠·à≥·àà·ä®·å•·äï·âµ·åÄ·àù·àÆ·åå·â≥·â†·àò·âÉ·â•·à≠·ã´·àâ·àô·â≥·äï·äï·àÅ·àâ·ä†·àµ·äê·à£·àÉ·ã≠·àõ·äñ·â∞·ä†...\n",
       "2200    niketechherasize4041424344madeinvietnamshewabr...\n",
       "843     givenchysize404142434445price:8000brfreedelive...\n",
       "Name: cleaned_message, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# üè∑Ô∏è Sample Messages for Manual NER Labeling (CoNLL Format)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Load cleaned messages\n",
    "cleaned_path = Path(\"data/cleaned/telegram_messages_cleaned.csv\")\n",
    "df = pd.read_csv(cleaned_path)\n",
    "\n",
    "# Drop any duplicates and short/empty messages\n",
    "df = df.drop_duplicates(subset=\"cleaned_message\")\n",
    "df = df[df[\"cleaned_message\"].str.len() > 10]\n",
    "\n",
    "# Sample 50 candidate messages (or fewer if limited)\n",
    "sampled = df.sample(n=min(50, len(df)), random_state=42)\n",
    "\n",
    "# Output path\n",
    "sample_path = Path(\"data/labeled/candidate_messages_for_labeling.txt\")\n",
    "sample_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save to plain text format (one message per line)\n",
    "sampled[\"cleaned_message\"].to_csv(sample_path, index=False, header=False)\n",
    "\n",
    "print(f\"üìù Sampled messages saved for labeling at: {sample_path.resolve()}\")\n",
    "sampled[\"cleaned_message\"].head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-extractor-challenge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
