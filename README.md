# B5W3: Insurance Risk Analytics & Predictive Modeling Week 3 - 10 Academy

## ğŸ—‚ Challenge Context
This repository documents the submission for 10 Academyâ€™s **B5W3: Insurance Risk Analytics & Predictive Modeling** challenge.
The goal is to support AlphaCare Insurance Solutions (ACIS) in optimizing underwriting and pricing by analyzing customer, vehicle, and claims data to:

- Identify low-risk customer segments

- Predict future risk exposure

- Enable data-driven premium optimization

This project simulates the role of a risk analyst at AlphaCare Insurance Solutions (ACIS), supporting actuarial and underwriting teams with data-driven insights for optimizing premium pricing and minimizing claims exposure.

The project includes:

- ğŸ§¹ Clean and structured ingestion of raw customer, vehicle, and claims datasets

- ğŸ“Š Multi-layered Exploratory Data Analysis (EDA) across customer, product, geographic, and vehicle dimensions

- ğŸ§  Modular profiling of loss ratio, outliers, and segment-specific profitability

- ğŸ—ƒï¸ Defensive schema auditing and data quality validation routines

- ğŸ“¦ Reproducible data versioning using DVC with Git and local cache integration

- ğŸ§ª Scaffolded modeling pipeline for classification-based claims risk prediction (planned)

- âœ… Structured orchestration of insights through testable, class-based Python modules and `eda_orchestrator.py` runner script


## ğŸ”§ Project Setup

To reproduce this environment:

1. Clone the repository:

```bash
git clone https://github.com/NabloP/b5w3-insurance-risk-modelling-challenge.git
cd b5w3-insurance-risk-modelling-challenge
```

2. Create and activate a virtual environment:
   
**On Windows:**
    
```bash
python -m venv insurance-challenge
.\insurance-challenge\Scripts\Activate.ps1
```

**On macOS/Linux:**

```bash
python3 -m venv insurance-challenge
source insurance-challenge/bin/activate
```

3. Install dependencies

```bash
pip install -r requirements.txt
```

## âš™ï¸ CI/CD (GitHub Actions)

This project uses GitHub Actions for Continuous Integration. On every `push` or `pull_request` event, the following workflow is triggered:

- Checkout repo

- Set up Python 3.10

- Install dependencies from `requirements.txt`

CI workflow is defined at:

    `.github/workflows/unittests.yml`

## ğŸ“ Project Structure

<!-- TREE START -->
ğŸ“ Project Structure

solar-challenge-week1/
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ pytest.ini
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ unittests.yml
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ cleaned/
â”‚   â”œâ”€â”€ mappings/
â”‚   â”‚   â”œâ”€â”€ label_encodings.csv
â”‚   â”œâ”€â”€ outputs/
â”‚   â”‚   â”œâ”€â”€ hypothesis_results.csv
â”‚   â”‚   â”œâ”€â”€ loss_ratio_bubble_map.png
â”‚   â”‚   â”œâ”€â”€ plots/
â”‚   â”‚   â””â”€â”€ segments/
â”‚   â”‚       â”œâ”€â”€ province_comparison.csv
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”œâ”€â”€ cleaned_insurance_data.csv
â”‚   â”‚   â”œâ”€â”€ enriched_insurance_data.csv
â”‚   â””â”€â”€ raw/
â”‚       â”œâ”€â”€ MachineLearningRating_v3.txt
â”‚       â”œâ”€â”€ MachineLearningRating_v3.txt.dvc
â”‚       â”œâ”€â”€ opendb-2025-06-17.csv
â”‚       â”œâ”€â”€ opendb-2025-06-17.csv.dvc
â”œâ”€â”€ docs/
â”œâ”€â”€ models/
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ task-1-eda-statistical-planning.ipynb
â”‚   â”œâ”€â”€ task-3-hypothesis-testing.ipynb
â”‚   â”œâ”€â”€ task-4-predictive-modeling.ipynb
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ eda_orchestrator.py
â”‚   â”œâ”€â”€ generate_tree.py
â”‚   â”œâ”€â”€ hypothesis_testing_orchestrator.py
â”‚   â”œâ”€â”€ predictive_modeling_orchestrator.py
â”‚   â”œâ”€â”€ version_datasets.py
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”œâ”€â”€ eda/
â”‚   â”‚   â”œâ”€â”€ defensive_schema_auditor.py
â”‚   â”‚   â”œâ”€â”€ distribution_analyzer.py
â”‚   â”‚   â”œâ”€â”€ gender_risk_profiler.py
â”‚   â”‚   â”œâ”€â”€ geo_risk_visualizer.py
â”‚   â”‚   â”œâ”€â”€ iqr_outlier_detector.py
â”‚   â”‚   â”œâ”€â”€ numeric_plotter.py
â”‚   â”‚   â”œâ”€â”€ plan_feature_risk_profiler.py
â”‚   â”‚   â”œâ”€â”€ schema_auditor.py
â”‚   â”‚   â”œâ”€â”€ schema_guardrails.py
â”‚   â”‚   â”œâ”€â”€ temporal_analyzer.py
â”‚   â”‚   â”œâ”€â”€ vehicle_risk_profiler.py
â”‚   â”œâ”€â”€ hypothesis_testing/
â”‚   â”‚   â”œâ”€â”€ data_cleaner.py
â”‚   â”‚   â”œâ”€â”€ group_segmenter.py
â”‚   â”‚   â”œâ”€â”€ hypothesis_tester.py
â”‚   â”‚   â”œâ”€â”€ metric_definitions.py
â”‚   â”‚   â”œâ”€â”€ visual_tester.py
â”‚   â””â”€â”€ modeling/
â”‚       â”œâ”€â”€ class_balancer.py
â”‚       â”œâ”€â”€ expected_premium_calculator.py
â”‚       â”œâ”€â”€ feature_scaler.py
â”‚       â”œâ”€â”€ logistic_model_trainer.py
â”‚       â”œâ”€â”€ random_forest_trainer.py
â”‚       â”œâ”€â”€ target_feature_builder.py
â”‚       â”œâ”€â”€ train_test_splitter.py
â”‚       â”œâ”€â”€ xgboost_model_trainer.py
â”‚       â”œâ”€â”€ xgboost_regressor_trainer.py
â”œâ”€â”€ tests/
â””â”€â”€ ui/
<!-- TREE END -->


## âœ… Status

- â˜‘ï¸ Task 1 complete: Full EDA pipeline implemented across 10 modular risk layers (loss ratio, outliers, geo, schema, etc.)

- â˜‘ï¸ Task 2 complete: DVC tracking initialized with Git integration, local remote configured, and raw datasets versioned

- â˜‘ï¸ Task 3 complete: Fully modular A/B testing pipeline implemented with group segmentation, adaptive t-test or Mannâ€“Whitney testing, effect size logging, and KPI visualizations. Province-level comparison between Western Cape and Gauteng now reproducible via `scripts/hypothesis_testing_orchestrator.py`.

- â˜‘ï¸ Task 4 complete: Predictive modeling pipeline with classification, regression, SHAP, and premium estimation

â˜‘ï¸ Project architecture: Fully modular `src/`, `scripts/`, and `notebooks/` structure with reproducible orchestration via `eda_orchestrator.py` and `version_datasets.py`


## ğŸ“¦ Key Capabilities

- âœ… Class-based, modular Python modules for full insurance ML workflow  
- âœ… Risk signal extraction with KPIs: `ClaimFrequency`, `ClaimSeverity`, `Margin`  
- âœ… Segment-aware A/B testing (by Province, Gender, etc.)  
- âœ… Explainable models using SHAP for both classification + regression  
- âœ… Premium engine: **Premium = P(Claim) Ã— E[ClaimAmount] + Margin**


## ğŸ“¦ What's in This Repo

This repository is structured to maximize modularity, reusability, and clarity:

- ğŸ“ Layered Python module structure for risk profiling (src/eda/), geographic mapping (src/geo/), and schema auditing (src/)

- ğŸ§ª CI-ready architecture using GitHub Actions for reproducible tests via pytest

- ğŸ“¦ DVC integration for versioned tracking of raw and processed datasets (with local remote and cache routing)

- ğŸ§¹ Clean orchestration scripts (eda_orchestrator.py, version_datasets.py) for Task 1â€“2 reproducibility

- ğŸ§  Risk analysis modules written with defensive programming, strong validation, and class-based design

- ğŸ“Š Insightful plots (loss ratio heatmaps, bar charts, outlier maps) auto-rendered via orchestrator pipeline

- ğŸ§¾ Consistent Git hygiene with .gitignore, no committed .csv or .venv, and modular commit history

- ğŸ§ª Notebook-first development approach supported by CLI runners and reusable core modules

- ğŸ§ª Statistical hypothesis testing orchestrator for A/B segment comparison across provinces, genders, or zip codes, powered by `GroupSegmenter`, `HypothesisTester`, and `VisualTester`


- ğŸ§  **My Contributions:** All project scaffolding, README setup, automation scripts, and CI configuration were done from scratch by me


## ğŸ” DVC Configuration & Versioning (Task 2)
This project uses **Data Version Control (DVC)** to ensure auditable and reproducible handling of insurance datasets across all preprocessing stages.

### âœ… Versioned Artifacts
The following DVC artifacts are tracked and committed to the repository:

File	| Purpose
--------|---------
data/raw/MachineLearningRating_v3.txt.dvc	| Tracks raw dataset (customer + claims)
data/raw/opendb-2025-06-17.csv.dvc	| Tracks auxiliary postal code metadata
.dvc/config	| Stores remote and cache settings
.gitignore	| Automatically updated to ignore large .csv files

Note: This project currently uses .dvc-style tracking (per file), not dvc.yaml pipelines. The dvc.yaml file will be added in Task 3â€“4 for full ML pipeline definition.

### ğŸ“¦ DVC Remote Configuration
DVC is configured to use a local remote directory (outside the Git repo) for safe, decoupled storage:

```swift

Remote path: C:/Users/admin/Documents/GIT Repositories/dvc_remote/.dvc/cache
```

This is specified in .dvc/config as:

```ini
['cache']
    dir = C:/Users/admin/Documents/GIT Repositories/dvc_remote/.dvc/cache
```

And confirmed via:

```bash
dvc config cache.dir "C:/Users/admin/Documents/GIT Repositories/dvc_remote/.dvc/cache"
```

### ğŸ” How to Push to DVC Remote

To sync all .dvc-tracked data to the configured local remote:

```bash
dvc add data/raw/MachineLearningRating_v3.txt
dvc add data/raw/opendb-2025-06-17.csv
git add data/raw/*.dvc .gitignore
git commit -m "Track raw datasets with DVC"
dvc push
```

### ğŸ”§ Automation Support

For reproducibility, all versioning steps can be automated using:

```bash
python scripts/version_datasets.py
```

This script:

- Adds tracked datasets to DVC

- Commits .dvc files to Git

- Pushes artifacts to remote

- Logs all actions to dvc_logs/





## ğŸ§ª Usage

This project supports both **script-based automation** and **notebook-driven workflows** across all four challenge tasks. Below are the primary orchestration scripts and how to use them.

---

### ğŸ”¬ Task 1 â€“ Full EDA Pipeline

Run the complete multi-layered risk EDA pipeline using:

    python scripts/eda_orchestrator.py

This script executes all 10 analytical layers in sequence:

Layer | Focus Area
------|------------
1     | Schema structure audit (duplicates, types, nulls)
2A    | Descriptive statistics (mean, std, skew, kurtosis)
2B    | Histogram and boxplot visualization
3     | Monthly temporal trend analysis
4     | Geographic loss ratio heatmap and bar charts
5     | Vehicle model and make risk profiling
6     | Gender-based loss segmentation
7     | Policy segment comparison (CoverType, TermFrequency, etc.)
8     | IQR-based outlier flagging
9     | Defensive schema diagnostics (cardinality, constants)
10    | Guardrail-based schema cleanup and exclusions

Outputs are **printed inline only** for review â€” no files are saved by default.

---

### ğŸ’¾ Task 2 â€“ Dataset Versioning with DVC

To version all raw datasets and push to the configured DVC remote, run:

    python scripts/version_datasets.py

This script:

- Verifies DVC initialization
- Adds each file to DVC tracking (.dvc pointers)
- Commits pointer files to Git
- Pushes all tracked data to the local remote
- Logs actions to: dvc_logs/dvc_versioning_log_<DATE>.txt

Tracked artifacts include:

- data/raw/MachineLearningRating_v3.txt
- data/raw/opendb-2025-06-17.csv

Remote path is set to:
C:/Users/admin/Documents/GIT Repositories/dvc_remote/.dvc/cache

---

### ğŸ§ª Task 3 â€“ Province-Level Hypothesis Testing

To statistically compare risk KPIs across **Western Cape vs Gauteng**, run:

    python scripts/hypothesis_testing_orchestrator.py

This script:

- Loads, cleans, and prepares insurance data
- Derives 3 KPIs:
    - ClaimFrequency
    - ClaimSeverity
    - Margin
- Segments data into A/B groups using province labels
- Applies adaptive statistical tests:
    - Mannâ€“Whitney U (nonparametric)
    - Independent t-test (parametric)
- Logs:
    - p-values
    - test method
    - effect size
    - group normality results
- Saves result table to:
    - data/outputs/hypothesis_results.csv
- Renders 3 visualizations per KPI:
    - Violin plot
    - Boxplot
    - Histogram overlay

Script can be extended to compare **Gender**, **ZipCode**, or any other categorical column by updating the GroupSegmenter.


### ğŸ¤– Task 4 â€“ Predictive Modeling Pipeline

```bash
python scripts/predictive_modeling_orchestrator.py
```

This script:
- Prepares modeling data (with feature engineering, outlier handling)
- Trains:
  - Logistic Regression
  - Random Forest
  - XGBoost (Classifier & Regressor)
- Evaluates models on:
  - F1, ROC-AUC (classification)
  - RMSE, RÂ² (regression)
- Runs SHAP explainability on the best model
- Computes expected premium:
  `Premium = P(Claim) Ã— E[Severity] + Margin`
- Saves all outputs to `data/outputs/`:
  - `model_performance_metrics.csv`
  - `predicted_premiums.csv`
  - `shap_summary.png`


## ğŸ§  Design Philosophy
This project was developed with a focus on:

- âœ… Modular Python design using classes, helper modules, and runners (clean script folders and testable code)
- âœ… High commenting density to meet AI and human readability expectations
- âœ… Clarity (in folder structure, README, and docstrings)
- âœ… Reproducibility through consistent Git hygiene and generate_tree.py
- âœ… Rubric-alignment (clear deliverables, EDA, and insights)

## ğŸš€ Author
Nabil Mohamed
AIM Bootcamp Participant
GitHub: [NabloP](https://github.com/NabloP)

# B5W4: Amharic E-Commerce Data Extractor Challenge â€“ 10 Academy

## ğŸ—‚ Challenge Context
This repository documents the submission for 10 Academyâ€™s **B5W4: Amharic E-Commerce Data Extractor Challenge**.  
The goal is to support EthioMart in becoming Ethiopiaâ€™s centralized hub for Telegram-based e-commerce by:

- Extracting key business entities (product, price, location) from unstructured Amharic Telegram messages  
- Fine-tuning transformer models for accurate Amharic NER  
- Scoring vendors based on their activity, reach, and pricing to enable data-driven micro-lending  

The project simulates the role of a fintech data analyst building a structured NLP pipeline for intelligent vendor profiling.

### Key Features
- ğŸ§² Real-time Telegram scraping of e-commerce messages and metadata  
- âœï¸ CoNLL-format labeling of Amharic text with Product, Price, and Location entities  
- ğŸ¤– Transformer-based fine-tuning (XLM-Roberta, mBERT) for NER extraction  
- ğŸ“Š Vendor-level analytics and micro-lending scorecards  
- ğŸ” Model explainability using SHAP and LIME  

---

## ğŸ”§ Project Setup

### 1. Clone the repository:
git clone https://github.com/NabloP/b5w4-amharic-ecommerce-data-extractor-challenge.git
cd b5w4-amharic-ecommerce-data-extractor-challenge

### 2. Create and activate a virtual environment:
**On Windows (PowerShell):**
python -m venv data-extractor-challenge
data-extractor-challenge\Scripts\Activate.ps1

**On macOS/Linux:**
python3 -m venv data-extractor-challenge
source data-extractor-challenge/bin/activate

### 3. Install dependencies:
pip install -r requirements.txt

---

## ğŸ“ Project Structure

<!-- TREE START -->
b5w4-amharic-ecommerce-data-extractor-challenge/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ cleaned/
â”‚   â”œâ”€â”€ labeled/
â”‚   â”œâ”€â”€ outputs/
â”‚   â””â”€â”€ logs/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ preprocessing/
â”‚   â”œâ”€â”€ labeling/
â”‚   â”œâ”€â”€ modeling/
â”‚   â”œâ”€â”€ evaluation/
â”‚   â””â”€â”€ analytics/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ ingest_data.py
â”‚   â”œâ”€â”€ label_data.py
â”‚   â”œâ”€â”€ fine_tune_model.py
â”‚   â”œâ”€â”€ evaluate_models.py
â”‚   â””â”€â”€ generate_scorecards.py
â”œâ”€â”€ notebooks/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ LICENSE
<!-- TREE END -->

---

## âœ… Status
- â˜‘ï¸ Task 1 complete: Ingested Telegram data from 5+ vendors, preprocessed and normalized Amharic messages  
- â˜‘ï¸ Task 2 complete: Manually labeled 50+ messages in CoNLL format  
- â¬œ Task 3: Fine-tuning underway using XLM-Roberta and Hugging Face pipeline  
- â¬œ Task 4: Model evaluation and comparison to be completed  
- â¬œ Task 5: SHAP & LIME interpretation pending  
- â¬œ Task 6: Vendor scorecard module in progress  

---

## ğŸ“¦ Key Capabilities
- âœ… Real-time ingestion from Telegram via Telethon  
- âœ… Amharic-friendly tokenization and text normalization  
- âœ… CoNLL-format labeling and entity alignment  
- âœ… Hugging Face-based model fine-tuning for NER  
- âœ… Scorecard logic to support lending decisions  

---

## ğŸ§  Design Philosophy
This project emphasizes:

- ğŸ“¦ Modular folder and code design (reusable Python classes)  
- ğŸ” Reproducibility via `requirements.txt` and consistent naming  
- ğŸ§ª Explainability using SHAP and LIME  
- ğŸ” Transparency in scoring logic for vendor prioritization  

---

## ğŸš€ Author
Nabil Mohamed  
10 Academy Bootcamp â€“ B5W4 Cohort  
GitHub: https://github.com/NabloP